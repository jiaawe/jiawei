import type { Project, Publication } from "../types";

export const projects: Project[] = [
  {
    id: 1,
    title: "Enhancing Sports Strategy with Video Analytics and Data Mining",
    description: "In my final-year project, I introduced a comprehensive video-based analytics framework for tennis doubles, featuring a standardised annotation methodology and specialised annotation tool. The tool is semi-automated, integrating advanced machine learning techniques like GroundingDINO for player localisation and YOLO-Pose for pose estimation as part of a two-stage inference process, significantly reducing manual annotation efforts. Experimental results showed CNN-based models with transfer learning outperformed pose-based approaches in predicting shot types, player positioning, and formations, creating a foundation for future strategic tennis analytics.",
    technologies: ["React", "Flask", "FFmpeg", "PyTorch", "GroundingDINO", "YOLO-Pose", "GCN", "CNN", "Transfer Learning"],
    githubUrl: "https://github.com/jiaawe/tennis-prediction",
    pdfUrl: "https://drive.google.com/file/d/1769hAjnJj3i3hTJpe_0buXBS8aJXEJ2V/view?usp=sharing",
    imageUrl: "tennis-annotation-framework", // This is now just an identifier, actual path handled in ProjectCard
    featured: true
  },
  {
    id: 2,
    title: "IntersectionGPT: Real-Time Traffic Control, Reimagined",
    description: "Together with several teammates, we have developed IntersectionGPT, an innovative AI-powered traffic management system that uses large language models to dynamically optimize traffic signals in real time. We implemented computer vision and real-time data analytics to monitor traffic flow, detect incidents, and prioritize emergency vehicles. In this project, we also created a simulation platform demonstrating up to 159% throughput improvement and 67% reduction in average vehicle delay compared to traditional fixed-interval systems. Our adaptive decision-making algorithms also factor in weather conditions, traffic density and unexpected incidents to improve urban intersection efficacy.",
    technologies: ["YOLOv8", "Kafka", "Flink", "AI Agents", "LLM Guardrails", "Real-time Processing"],
    githubUrl: "https://github.com/jiaawe/red-light-green-light",
    pdfUrl: "https://drive.google.com/file/d/1tyiS7EQZqJToUNaeKZ0ob46s9u7YFt7J/view?usp=sharing",
    imageUrl: "intersectiongpt", // This is now just an identifier, actual path handled in ProjectCard
    featured: true
  },
  {
    id: 3,
    title: "Kuaishou-Recommender: Short-Form Video Recommendation System",
    description: "We developed a video recommendation system using the open-sourced, fully observed KuaiRec dataset. Our approach involved a hybrid model combining Neural Collaborative Filter (NCF) and Caption-based embedding techniques, to leverage the strengths of both methods. We ran a regression analysis to evaluate the performance of our models, and our results indicated that our NCF-based model combining time decay, matrix factorisation and multi-layer perceptron layers greatly outperformed the baseline models. We evaluated the performances with various relevance metrics (precision@k, recall@k, F1-score@k) and diversity metrics (Category diversity, Category-Aware NDCG).",
    technologies: ["Recommendaton Systems", "Neural Collaborative Filtering", "Matrix Factorization", "Deep Learning", "Text Mining"],
    githubUrl: "https://github.com/jiaawe/kuaishou-recommender",
    pdfUrl: "https://drive.google.com/file/d/1eWXvjU4YgucM1A6zlWVzNGs7yna6DkKC/view?usp=sharing",
    imageUrl: "kuaishou-recommender", // This is now just an identifier, actual path handled in ProjectCard
    featured: true
  },
  {
    id: 4,
    title: "Deepfake Detection: Can Detect Or Not Ah?",
    description: "As part of a hackathon, my team and I developed a deepfake detection system for images, audio & videos. For images, we leveraged open-source generative models (GANs, VAEs, SDs) to create a datset of deepfakes, and trained a custom CNN model to detect them. For videos, we sampled frames sufficiently diverse across the video and used the same CNN model for aggregated predictions. Lastly, for audio, we used Mel-frequency cepstral coefficients (MFCCs) to extract audio features before feeding them into a tree-based model. We achieved impressive recall, precision and F1-scores across millions of open-source deepfake images, proving that we generalise the detection of deepfakes with a synthetic dataset generated by existing state-of-the-art generative models.",
    technologies: ["Deepfake Detection", "Audio Processing", "CNN", "MFCCs", "Generative Models"],
    githubUrl: "http://github.com/jiaawe/Deepfake-Detector",
    pdfUrl: "https://github.com/jiaawe/Deepfake-Detector/blob/main/README.md",
    imageUrl: "deepfake", // This is now just an identifier, actual path handled in ProjectCard
    featured: true
  },
];

export const publications: Publication[] = [
  {
    id: 1,
    title: "Modeling Formation Strategies in Tennis Doubles Games",
    authors: ["Zhaoyu Liu", "Chen Dong", "Jia Wei Chen", "Alvin Min Jun Jiang", "Guanzhou Chen", "Aayan Faraz Shaikh", "Tian Yu Dong", "Chen Wang", "Kan Jiang", "Jin Song Dong"],
    journal: "SN Computer Science",
    year: 2024,
    abstract: "In the dynamic and strategic environment of tennis doubles games, understanding the multifaceted interactions between players is crucial for enhancing team performance. In our previous work, we introduced a novel analytical framework for tennis doubles, employing Markov Decision Processes (MDP) and probabilistic model checking (PMC) to model the intricate behaviors and interactions in the doubles game. Our previous model only considered the standard standing formation. However, in the professional and NCAA Division 1 doubles matches, the \"I\" formation is utilized very often but was missing from our previous work. In this paper, we aim to extend our previous model with different formations and discuss the effectiveness of various formation strategies.",
    link: "https://link.springer.com/article/10.1007/s42979-024-03598-3"
  }
];